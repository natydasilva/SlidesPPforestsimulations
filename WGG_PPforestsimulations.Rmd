---
title: <span style="color:#000000"> Projection pursuit classification random forests  </span>
author: "Natalia da Silva"
date: "March 31, 2017"
output: ioslides_presentation
bibliography: bibliophd.bib
---


 <style>
 .title-slide {
     background-image: url(forest.jpg);
     background-repeat: no-repeat;
     padding:40px;
     background-size: 1000px 800px;
   }
   </style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## <span style="color:#26734d">Structure</span> 
-Motivation

-Ensemble models

-PPtree

-PPforest

-Simulations results

-Maybe some diagnostics for PPforest paper advances

## <span style="color:#26734d">Motivation</span> 
**PPforest** is a new method based on bagged projection pursuit trees for classification problems.

This method was designed to work well in cases were the variables are highly correlated and the separation between classes occur on combinations of variables. 

## <span style="color:#26734d"> Ensemble models</span> 
- Ensembles learning methods: combined multiple individual models trained independently to build a prediction model.

- Some well known examples of ensemble learning methods are, boosting [@schapire1990strength], bagging [@breiman1996bagging] and random forest [@breiman2001random] among others.

-Main differences between ensembles, type of individual models to be combined and  the ways these individual models are combined. 

##<span style="color:#26734d">PPforest</span>  
Projection pursuit classification random forest [@dasilvappforest] is an ensemble learning method, built on bagged trees.  

Main concepts:
-Bootstrap aggregation (\cite{breiman1996bagging} and \cite{breiman1996heuristics})
-Random feature selection (\cite{amit1997shape} and \cite{ho1998random}) to individual classification trees for prediction.


##<span style="color:#26734d">PPforest, individual classifiers</span>  
The individual classifier in **PPforest** is a **PPtree** [@lee2005projection].

The splits in **PPforest** are based on a linear combination of randomly chosen variables.
Utilizing linear combinations of variables the individual model (PPtree) separates classes taking into account the correlation between variables.

## <span style="color:#26734d"> PPtree </span> 
<!-- Combines tree structure methods with projection pursuit dimension reduction. -->
Treats the data always as a two-class system,  when the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.

<!-- 1. Optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data.  -->
<!-- 2.With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $``g1"$ or $``g2"$ to each observation, a new variable $y_i^*$ is created.  The new groups ``g1'' and ``g2'' can contain more than one original classes.  -->
<!-- 3. Re-do PP, find optimal one-dimensional projection $\alpha$, using $\{(\mathbf{x_i},y_i^*)\}_{i=1}^n$ to separate the two class problem $g1$ and $g2$. -->

<!-- The best separation of $g1$ and $g2$ is determine in this step and the decision rule is defined for the current node, if $\alpha^T M1< c$ then assign ``g1'' to the left node else assign ``g2'' to the right node, where $M1$ is the mean of ``g1''. -->
<!-- For each groups we can repeat all the previous steps until $g1$ and $g2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes. -->

## <span style="color:#26734d"> PPtree algorithm </span> 
1. In each node a PP index is optimized to find the optimal $1-D$ projection, $\alpha^*$, for separating all classes in the current data.
2. Reduce the number of classes to two, by comparing means and assign new labels, $g_1$ or $g_2$ ($y_i^*$) to each observation.
3. Re-do PP with these new group labels finding the $1-D$ projection, $\alpha$ using $(x_i,y^*)$.
4. Calculate the decision boundary c, keep $\alpha$ and $c$.
5.  Separate data into two groups using new group labels $g_1$ and $g_2$.
6.  For each group, stop if there is only one class else repeat the procedure, the splitting steps are iterated until the last two classes are separated.


##<span style="colorL#26734d">PPtree example simulated data</span>
```{r libs, echo = FALSE, warnin = FALSE, message=FALSE,fig.align="center"}
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(PPtreeViz)
library(gridExtra)
library(reshape2)
library(PPforest)
library(plyr)
library(dplyr)

```
<center>
```{r, echo = FALSE, fig.height = 7, fig.width = 7, fig.align = "center", cache = TRUE}
simu3 <- function(mux1, mux2, muy1, muy2, muz1, muz2,  cor1,cor2,cor3) {
  set.seed(666)
  bivn <- mvrnorm(100, mu = c(mux1, mux2), Sigma = matrix(c(1, cor1, cor1, 1), 2))
  bivn2 <- mvrnorm(100, mu = c(muy1, muy2), Sigma = matrix(c(1, cor2, cor2, 1), 2))
  bivn3 <- mvrnorm(100, mu = c(muz1, muz2), Sigma = matrix(c(1, cor3, cor3, 1), 2))

  d1 <- data.frame(Sim = "sim1", bivn)
  d2 <- data.frame(Sim = "sim2", bivn2)
  d3 <- data.frame(Sim = "sim3", bivn3)
  rbind(d1, d2, d3)
}



dat.pl2 <- simu3(-1,0.6,0,-0.6, 2,-1,0.95, 0.95, 0.95)



grilla <- base::expand.grid(X1 = seq(-4,4.8,,100), X2 = seq(-4.3,3.3,,100))

pptree <- PPtreeViz::PPTreeclass(Sim~., data = dat.pl2, "LDA")
ppred.sim <- PPtreeViz::PPclassify(pptree, test.data = grilla, Rule = 1)
grilla$ppred<-ppred.sim[[2]]

rpart.crab <- rpart::rpart(Sim ~ X1 + X2, data = dat.pl2)
rpart.pred <- predict(rpart.crab, newdata = grilla, type="class")

p <- ggplot2::ggplot(data = grilla ) +
  ggplot2::geom_point(ggplot2::aes(x = X1, y = X2, color = as.factor(ppred),shape=as.factor(ppred)),alpha = .20)+
  geom_abline(intercept= pptree$splitCutoff.node[[1]]/pptree$projbest.node[[3]], slope= -pptree$projbest.node[[1]]/pptree$projbest.node[[3]], size=1 )+ scale_colour_brewer(name="Class",type="qual",palette="Dark2")+ggplot2::theme_bw() +
  geom_abline(intercept= pptree$splitCutoff.node[[2]]/pptree$projbest.node[[4]], slope=-pptree$projbest.node[[2]]/pptree$projbest.node[[4]],size=1)+ scale_shape_discrete(name='Class')

pl.pp <- p + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group= Sim, shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1) + scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))

p2 <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x = X1, y = X2  , color = as.factor(rpart.pred),shape =  as.factor(rpart.pred)), alpha = .2) +
  ggplot2::scale_colour_brewer(name = "Class",labels = levels(dat.pl2$Sim),type="qual",palette="Dark2") +
  ggplot2::theme_bw() + scale_shape_discrete(name='Class')+ scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))

pl.rpart <- p2 + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group=Sim,shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1)

grid.arrange(pl.rpart,pl.pp,ncol=2)

```
</center>

##<span style="color:#26734d">PPforest</span>  
Projection pursuit classification random forest [@dasilvappforest] is an ensemble learning method, built on bagged trees.  

Main concepts:

* Bootstrap aggregation (\cite{breiman1996bagging} and \cite{breiman1996heuristics}) 

* Random feature selection (\cite{amit1997shape} and \cite{ho1998random}) to individual classification trees for prediction.

The splits in PPforest are based on a linear combination of randomly chosen variables.
Utilizing linear combinations of variables the individual model (PPtree) separates classes taking into account the correlation between variables.


##<span style="color:#26734d">PPforest algorithm</span> 


1. Input: $L=\{(x_i,y_i), i=1,... n\}$,  $\ y_i\in \{1,..., g\}$ where $y_i$ is the class information
2. Draw $b=1,\ldots, B$ bootstrap samples, $L^{*b}$ of size $n$ from $L$
3. For each bootstrap sample grow a PPtree classifier $T^{*b}$ and for every node a sample of m variables without replacement is drawn.
4. Predict the classes of each case not included in $L^*$ and compute the oob error.
5. Based on majority vote predict the class in a new data set.

##<span style="color:#26734d">Simulations</span>

The purpose is to determine where **PPforest** beats **randomForest**, hopefully somewhere.

This method should work better in cases were the variables are highly correlated and the separation between classes occur on combinations of variables. 


##<span style="color:#26734d">Defferent approaches</span>

Three simulation approaches

*Response based on theoretical quantiles of $\sum_{i=1}^p X_i$ 
<!-- Response variable based on theoretical quantile from a Normal distribution. -->
<!-- Simulated data are not used to define classes but its distribution is used to define them. Classes of the response are defined by $\sum_i^p X_i$. -->
<!-- Main advantage of this approach is it is easily generalized to any number of groups. -->

* Response based on theoretical quantiles of the PCA
<!-- Response variable based on theoretical quantile from a Normal distribution. -->
<!-- Simulated data are not used to define classes but its distribution is used to define classes. -->
<!-- Simulated data are not used to define classes but its distribution is used to define them. Classes of the response are defined by  $e_2^2 X$ ($e_2$ second eigen vector of $\Sigma$). -->
<!-- Main advantage of this approach is it is easily generalized to any number of groups. -->

* Separate simulation for each group

##<span style="color:#26734d">Response based on theoretical quantiles of $\sum_{i=1}^p X_i$  </span>


##<span style="color:#26734d">Response based on theoretical quantiles of the PCA</span>


## <span style="color:#26734d">Separate simulation for each group</span>

## <span style="color:#26734d">info</span>


1. shinyapp example:https://natydasilva.shinyapps.io/shinyppforest
2. Slides: https://github.com/natydasilva/slides
3. PPforest package:https://github.com/natydasilva/PPforest
4. email: ndasilva@iastate.edu
5. twitter : https://twitter.com/pacocuak



## Bibliography

