---
title: <span style="color:#000000"> Projection pursuit classification random forests  </span>
author: "Natalia da Silva"
date: "March 31, 2017"
output: ioslides_presentation
bibliography: bibliophd.bib
---


 <style>
 .title-slide {
     background-image: url(forest.jpg);
     background-repeat: no-repeat;
     padding:40px;
     background-size: 1000px 800px;
   }
   </style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## <span style="color:#26734d">Structure</span> 
-Motivation

-Ensemble models

-PPforest

-Simulations results


## <span style="color:#26734d">Motivation</span> 
**PPforest** is a new method based on bagged projection pursuit trees for classification problems.

This method was designed to work well in cases were the variables are highly correlated and the separation between classes occur on combinations of variables. 

## <span style="color:#26734d"> Ensemble models</span> 
- Ensembles learning methods: combined multiple individual models trained independently to build a prediction model.

- Some well known examples of ensemble learning methods are, boosting [@schapire1990strength], bagging [@breiman1996bagging] and random forest [@breiman2001random] among others.

-Main differences between ensembles, type of individual models to be combined and  the ways these individual models are combined. 

##<span style="color:#26734d">PPforest</span>  
Projection pursuit classification random forest [@dasilvappforest] is an ensemble learning method, built on bagged trees.  

Main concepts:

* Bootstrap aggregation (@breiman1996bagging and @breiman1996heuristics)

* Random feature selection (@amit1997shape and @ho1998random) to individual classification trees for prediction.


##<span style="color:#26734d">PPforest, individual classifiers</span>  
The individual classifier in **PPforest** is a **PPtree** [@lee2005projection].

The splits in **PPforest** are based on a linear combination of randomly chosen variables.
Utilizing linear combinations of variables the individual model (**PPtree**) separates classes taking into account the correlation between variables.

## <span style="color:#26734d"> PPtree, individual classifier for PPforest  </span> 
<!-- Combines tree structure methods with projection pursuit dimension reduction. -->
Treats the data always as a two-class system.

When the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split.

<!-- 1. Optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data.  -->
<!-- 2.With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $``g1"$ or $``g2"$ to each observation, a new variable $y_i^*$ is created.  The new groups ``g1'' and ``g2'' can contain more than one original classes.  -->
<!-- 3. Re-do PP, find optimal one-dimensional projection $\alpha$, using $\{(\mathbf{x_i},y_i^*)\}_{i=1}^n$ to separate the two class problem $g1$ and $g2$. -->

<!-- The best separation of $g1$ and $g2$ is determine in this step and the decision rule is defined for the current node, if $\alpha^T M1< c$ then assign ``g1'' to the left node else assign ``g2'' to the right node, where $M1$ is the mean of ``g1''. -->
<!-- For each groups we can repeat all the previous steps until $g1$ and $g2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes. -->

## <span style="color:#26734d"> PPtree algorithm </span> 
1. In each node a PP index is optimized to find the optimal $1-D$ projection, $\alpha^*$, for separating all classes in the current data.
2. Reduce the number of classes to two, by comparing means and assign new labels, $g_1$ or $g_2$ ($y_i^*$) to each observation.
3. Re-do PP with these new group labels finding the $1-D$ projection, $\alpha$ using $(x_i,y^*)$.
4. Calculate the decision boundary c, keep $\alpha$ and $c$.
5.  Separate data into two groups using new group labels $g_1$ and $g_2$.
6.  For each group, stop if there is only one class else repeat the procedure, the splitting steps are iterated until the last two classes are separated.


##<span style="colorL#26734d">PPtree example simulated data</span>
```{r libs, echo = FALSE, warnin = FALSE, message=FALSE,fig.align="center"}
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(PPtreeViz)
library(gridExtra)
library(reshape2)
library(PPforest)
library(plyr)
library(dplyr)
library(GGally)

```
<center>
```{r, echo = FALSE, fig.height = 7, fig.width = 7, fig.align = "center", cache = TRUE}
simu3 <- function(mux1, mux2, muy1, muy2, muz1, muz2,  cor1,cor2,cor3) {
  set.seed(666)
  bivn <- mvrnorm(100, mu = c(mux1, mux2), Sigma = matrix(c(1, cor1, cor1, 1), 2))
  bivn2 <- mvrnorm(100, mu = c(muy1, muy2), Sigma = matrix(c(1, cor2, cor2, 1), 2))
  bivn3 <- mvrnorm(100, mu = c(muz1, muz2), Sigma = matrix(c(1, cor3, cor3, 1), 2))

  d1 <- data.frame(Sim = "sim1", bivn)
  d2 <- data.frame(Sim = "sim2", bivn2)
  d3 <- data.frame(Sim = "sim3", bivn3)
  rbind(d1, d2, d3)
}



dat.pl2 <- simu3(-1,0.6,0,-0.6, 2,-1,0.95, 0.95, 0.95)



grilla <- base::expand.grid(X1 = seq(-4,4.8,,100), X2 = seq(-4.3,3.3,,100))

pptree <- PPtreeViz::PPTreeclass(Sim~., data = dat.pl2, "LDA")
ppred.sim <- PPtreeViz::PPclassify(pptree, test.data = grilla, Rule = 1)
grilla$ppred<-ppred.sim[[2]]

rpart.crab <- rpart::rpart(Sim ~ X1 + X2, data = dat.pl2)
rpart.pred <- predict(rpart.crab, newdata = grilla, type="class")

p <- ggplot2::ggplot(data = grilla ) +
  ggplot2::geom_point(ggplot2::aes(x = X1, y = X2, color = as.factor(ppred),shape=as.factor(ppred)),alpha = .20)+
  geom_abline(intercept= pptree$splitCutoff.node[[1]]/pptree$projbest.node[[3]], slope= -pptree$projbest.node[[1]]/pptree$projbest.node[[3]], size=1 )+ scale_colour_brewer(name="Class",type="qual",palette="Dark2")+ggplot2::theme_bw() +
  geom_abline(intercept= pptree$splitCutoff.node[[2]]/pptree$projbest.node[[4]], slope=-pptree$projbest.node[[2]]/pptree$projbest.node[[4]],size=1)+ scale_shape_discrete(name='Class')

pl.pp <- p + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group= Sim, shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1) + scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))

p2 <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x = X1, y = X2  , color = as.factor(rpart.pred),shape =  as.factor(rpart.pred)), alpha = .2) +
  ggplot2::scale_colour_brewer(name = "Class",labels = levels(dat.pl2$Sim),type="qual",palette="Dark2") +
  ggplot2::theme_bw() + scale_shape_discrete(name='Class')+ scale_y_continuous(expand = c(0,0)) + scale_x_continuous(expand = c(0,0))

pl.rpart <- p2 + ggplot2::geom_point(data = dat.pl2, ggplot2::aes(x = X1 , y = X2, group=Sim,shape = Sim, color=Sim), size = I(3)  ) + theme(legend.position = "bottom", legend.text = element_text(size = 6), aspect.ratio = 1)

grid.arrange(pl.rpart,pl.pp,ncol=2)

```
</center>

<!-- ##<span style="color:#26734d">PPforest</span>   -->
<!-- Projection pursuit classification random forest [@dasilvappforest] is an ensemble learning method, built on bagged trees.   -->

<!-- Main concepts: -->

<!-- * Bootstrap aggregation (@breiman1996bagging and @breiman1996heuristics)  -->

<!-- * Random feature selection (@amit1997shape and @ho1998random) to individual classification trees for prediction. -->

<!-- The splits in PPforest are based on a linear combination of randomly chosen variables. -->
<!-- Utilizing linear combinations of variables the individual model (PPtree) separates classes taking into account the correlation between variables. -->


##<span style="color:#26734d">PPforest algorithm</span> 


1. Input: $L=\{(x_i,y_i), i=1,... n\}$,  $\ y_i\in \{1,..., g\}$ where $y_i$ is the class information
2. Draw $b=1,\ldots, B$ bootstrap samples, $L^{*b}$ of size $n$ from $L$
3. For each bootstrap sample grow a PPtree classifier $T^{*b}$ and for every node a sample of m variables without replacement is drawn.
4. Predict the classes of each case not included in $L^*$ and compute the oob error.
5. Based on majority vote predict the class in a new data set.


##<span style="color:#26734d">PPforest loves crab data!!</span> 


Measurements on rock crabs, 200 observations. 4 classes species-sex. 

* FL the size of the frontal lobe length, in mm
* RW rear width, in mm
* CL length of mid-line of the carapace, in mm
* CW maximum width of carapace, in mm
* BD depth of the body; for females, measured after displacement of the abdomen, in mm

##<span style="color:#26734d">Crab data</span> 

<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE}

 GGally::ggpairs(PPforest::crab,
    columns= 2:6,
    ggplot2::aes(colour=Type,alpha=.3),
    lower=list(continuous='points'),
    axisLabels='none',
    upper=list(continuous='blank')
     , legend = NULL)
```

##<span style="color:#26734d">PPforest vs RF</span>
```{r, chache=TRUE, size="small",echo=FALSE}
 rf.crab <- randomForest::randomForest(Type~., data = crab, 
              proximity = TRUE, ntree = 200)
pprf.crab <- PPforest(data = crab, class = "Type",
 std = TRUE, size.tr = 1, m = 200,
 size.p = .5, PPmethod = 'LDA' )
```

```{r, chache=TRUE, size="tiny", message=FALSE}

rf.crab
```
##<span style="color:#26734d">PPforest vs RF</span>
```{r, chache=TRUE, size="tiny", message=FALSE}
pprf.crab 
```

##<span style="color:#26734d">Simulations</span>

The purpose is to determine where **PPforest** works well.

<!-- beats **randomForest**, hopefully somewhere. -->

This method should work better in cases were the variables are highly correlated and the separation between classes occur on combinations of variables. 

We will check where **PPforest** beats **randomForest**

##<span style="color:#26734d">Defferent approaches</span>

Three simulation approaches

* Response based on theoretical quantiles of $\sum_{i=1}^p X_i$ 
<!-- Response variable based on theoretical quantile from a Normal distribution. -->
<!-- Simulated data are not used to define classes but its distribution is used to define them. Classes of the response are defined by $\sum_i^p X_i$. -->
<!-- Main advantage of this approach is it is easily generalized to any number of groups. -->

* Response based on theoretical quantiles of the PCA
<!-- Response variable based on theoretical quantile from a Normal distribution. -->
<!-- Simulated data are not used to define classes but its distribution is used to define classes. -->
<!-- Simulated data are not used to define classes but its distribution is used to define them. Classes of the response are defined by  $e_2^2 X$ ($e_2$ second eigen vector of $\Sigma$). -->
<!-- Main advantage of this approach is it is easily generalized to any number of groups. -->

* Separate simulation for each group, separation between groups in one direction

##<span style="color:#26734d">Simulation 1  </span>


* $X \sim N_{p}(\mu, \Sigma)$ with $\mu = (0,\ldots,0)$, $(\Sigma)_{ii} =1$ and $(\Sigma)_{ij} =\rho$.
$Z_i = \sum_{j = 1}^p X_{ij}$
* We use the theoretic quantiles of $Z_i$ to simulate the response.
Ej, if $G=2$ then $Y_i = I(Z_i> 0)$ and in a similar way using $Z_i$ quantiles for more groups.

Important: I'm not using the data to construct Y.

Advantage: easy to generalize to $g$ groups.

Drawback: correlation among $X$ and linear combination split are in opposite directions.


##<span style="color:#26734d">Simulation 1, scenarios  </span>
<!-- \begin{center} -->
<!--   \begin{tabular}{|c  | c |} -->
<!--     \hline -->
<!--     \bf{Parameters} & \bf{Values}  \\ \hline -->
<!--     $\rho$ & $\{0, 0.3, 0.6, 0.9\}$\\ \hline -->
<!--     Classes & $\{2, 4, 8\}$\\ \hline -->
<!--     Predictors & $\{5, 50, 100\}$ \\ \hline -->
<!--     Replicates & $20$\\ \hline -->
<!--     n & 200\\ -->
<!--     \hline -->
<!--   \end{tabular} -->
<!-- \end{center} -->

Parameter   | Value                  |
---------   | -------                |
$\rho$      | $\{0, 0.3, 0.6, 0.9\}$ |
Classes     | $\{2, 4, 8\}$          |
Predictors  | $\{5, 50, 100\}$       |
Replicates  | $20$                   |

##<span style="color:#26734d">Simulation 1, data with $\rho=0$  </span>
<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}
library(MASS)
library(ggplot2)
library(plyr)
library(dplyr)
library(RColorBrewer)
library(PPtreeViz)
library(gridExtra)
library(reshape2)
library(PPforest)
library(randomForest)
library(plotly)


simu1 <- function( rep, rho, p, class, n, theta = 0){

  Sigma1 <- matrix(rho, p, p, byrow = T)
  diag(Sigma1) <- 1
  dat1 <- as.data.frame(mvrnorm(n = n, rep(0,p), Sigma1))
  #simulate classes using the theoric quantiles of the  sum of the explanatory variables to get balanced data and separation
  #within classes based on linear combination of variables

  B <- rep(1, p)
  qnt2 <- qnorm(seq(0, 1, 1/class ), 0, sqrt(t(B)%*%Sigma1%*%(B)))
  y2 <- as.factor(cut(apply(dat1, 1,sum), breaks = qnt2, include.lowest = TRUE))
  levels(y2) <- as.factor(1:class)
  sim1 <- cbind(y2,dat1)

  if(theta!=0){
  R <-  rotmat(theta, p)
  sim1[ , -1] <- t(apply(sim1[,-1], 1, function(x) R%*%x)) %>% as.data.frame()
}
  return(sim1)
}
opi <- expand.grid( rho = c(0, 0.3, 0.6, 0.9),cl = c(2,4,8),
                    p = c(5, 50, 100), n = 200,  rep = 1)

set.seed(123)
simu.sce1 <- mlply(opi, simu1)
aux <- ldply(simu.sce1, function(x){
  y2 = x[,1]
  Z = apply(x[,-1],1,sum)
 if(ncol(x)-1==5){
   W= apply(x[,-1],1,function(w) w%*%c(1,1, 0,-1,-1))
 }else{
  W = apply(x[,-1], 1, function(w) w%*%rep(c(1,-1),each =(length(w))/2))

 }
  return(data.frame(y2,Z,W))
})


aux %>% filter(rho==0) %>%ggplot(  aes( y = W, x = Z, color = y2) ) + geom_point(alpha=.4, size=I(2))  + facet_grid(cl~p , labeller = label_both) + theme(legend.position = "bottom")+
  scale_colour_brewer(name="Class",type="qual",palette="Dark2") +labs(y="Z'W=0", x="Z")



```
</center>

##<span style="color:#26734d">Simulation 1, data with $p=50$  </span>
<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}

aux %>% filter(p==50) %>%ggplot(  aes( y = W, x = Z, color = y2) ) + geom_point(alpha=.4, size=I(2))  + facet_grid(cl~rho , labeller = label_both)+ theme(legend.position = "bottom")+
  scale_colour_brewer(name="Class",type="qual",palette="Dark2") +labs(y="W s.t Z'W=0", x="Z")
```
</center>

##<span style="color:#26734d">Simulation 1, results  </span>
<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}

load("simall1.Rdata")
mean_all1 <- plyr::ddply( all1,.( rho, cl, p, n,  Model), function(x) mean(x$V1) )
sd_all1 <- plyr::ddply( all1,.(rho, cl, p, n, Model), function(x) sd(x$V1) )
pl_all1 <- merge( mean_all1, sd_all1, by = c("rho", "cl", "p", "n", "Model") )


ggplot( data = pl_all1, aes( y = V1.x, x = rho, color = Model ) ) + geom_line()  + facet_grid(cl~p , labeller = label_both)+geom_errorbar(aes(ymin = V1.x - V1.y, ymax = V1.x + V1.y), width = .1) + labs(x = "Correlation", y = "Mean OOB error ") + theme(legend.position = "bottom")


```
</center>

##<span style="color:#26734d">Simulation 2</span>

*  $X \sim N_{p}(\mu, \Sigma)$ with $\mu = (0,\ldots,0)$, $(\Sigma)_{ii} =1$ and $(\Sigma)_{ij} =\rho$.
*  Using PCA a new variable $Z_i=\sum_{i=1}^p e_{2_i}X_i$ where $e_2$ is the second eigen vector of $\Sigma$.
Since $X\sim N(.,.)$ then $Z\sim N(.,.)$
*  $\sigma^*=e_2^T\Sigma e_2$   then $Z_i\sim N(0,\sigma^*)$
* We use the theoretic quantiles to simulate the response, if $G=2$ then $Y_i = I(Z_i > 0 )$ and in a similar way using $Z_i$ quantiles if the groups are 4 and so on.

Important: I'm not using the data to construct Y.

Advantage: easy to generalize to $g$ groups. Correlation among $X$ and linear combination split are in the same direction.

Drawback: I don't sure how to justify this construction 




##<span style="color:#26734d">Simulation 2, scenarios  </span>

Parameter   | Value                  |
---------   | -------                |
$\rho$      | $\{0, 0.3, 0.6, 0.9\}$ |
Classes     | $\{2, 4, 8\}$          |
Predictors  | $\{5, 50, 100\}$       |
Replicates  | $20$                   |


##<span style="color:#26734d">Simulation 2, data  $\rho=0$ </span>
<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}
simuacp <- function( rep, rho, p, class, n, comp = 2, theta = 0){

  Sigma1 <- matrix(rho, p, p, byrow = T)
  diag(Sigma1) <- 1
  dat1 <- as.data.frame(mvrnorm(n = n, rep(0,p), Sigma1))

  e <- eigen(Sigma1)$vectors[,comp]
  dat1acp <-as.matrix(dat1)%*%e

  qnt2 <- qnorm(seq(0, 1, 1/class ), 0, sqrt(t(e)%*%Sigma1%*%(e)))
  y2 <- as.factor(cut(dat1acp, breaks = qnt2, include.lowest = TRUE))

  levels(y2) <- as.factor(1:class)
  sim1 <- cbind(y2,dat1)

  if(theta!=0){
    R <-  rotmat(theta, p)
    sim1[ , -1] <- t(apply(sim1[,-1], 1, function(x) R%*%x)) %>% as.data.frame()
  }
  return(sim1)
}

simuacpaux <- function( rep, rho, p, class, n, comp = c(1,2)){

  Sigma1 <- matrix(rho, p, p, byrow = T)
  diag(Sigma1) <- 1
  dat1 <- as.data.frame(mvrnorm(n = n, rep(0,p), Sigma1))

  e <- eigen(Sigma1)$vectors[,comp]
  dat1acp <-as.matrix(dat1)%*%e

  qnt2 <- qnorm(seq(0, 1, 1/class ), 0, sqrt(t(e[,2])%*%Sigma1%*%(e[,2])))
  y2 <- as.factor(cut(dat1acp[,2], breaks = qnt2, include.lowest = TRUE))

  levels(y2) <- as.factor(1:class)
  sim1 <- data.frame(y2,dat1, dat1acp)

  
  return(sim1)
}

opi <- expand.grid( rho = c(0, 0.3, 0.6, 0.9),cl = c(2,4,8),
                    p = c(5, 50, 100), n = 200,  rep = 1)

set.seed(123)
simu.sceacp <- mlply(opi, simuacpaux)

aux <- ldply(simu.sceacp, function(x){
  y2 <- x[,1]
  
  Z <- x$X2
 W <- x$X1
  return(data.frame(y2,Z,W))
})


aux %>% filter(rho==0) %>%ggplot(  aes( y = W, x = Z, color = y2) ) + geom_point(alpha=.4, size=I(2))  + facet_grid(cl~p , labeller = label_both) + theme(legend.position = "bottom")+
  scale_colour_brewer(name="Class",type="qual",palette="Dark2") +labs(y="W s.t Z'W=0", x="Z")

```
</center>

## <span style="color:#26734d">Simulation 2, data with $p=50$</span>
<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}
aux %>% filter(p==50) %>%ggplot(  aes( y = Z, x = W, color = y2) ) + geom_point(alpha=.4, size=I(2))  + facet_grid(cl~rho , labeller = label_both) + theme(legend.position = "bottom")+
  scale_colour_brewer(name="Class",type="qual",palette="Dark2") +labs(y="Z'W=0", x="Z")
```
</center>

## <span style="color:#26734d">Simulation 2, results</span>

<center>
```{r, echo = FALSE, fig.height = 5, fig.width = 5, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}

load("simallacp.Rdata")

mean_all1 <- ddply( all1,.( rho, cl, p, n,  Model), function(x) mean(x$V1) )
sd_all1 <- ddply( all1,.(rho, cl, p, n, Model), function(x) sd(x$V1) )
pl_all1 <- merge( mean_all1, sd_all1, by = c("rho", "cl", "p", "n", "Model") )


ggplot( data = pl_all1, aes( y = V1.x, x = rho, color = Model ) ) + geom_line()  + facet_grid(cl~p , scales = "free_y", labeller = label_both)+geom_errorbar(aes(ymin = V1.x - V1.y, ymax = V1.x + V1.y), width = .1) + labs(x = "Correlation", y = "Mean OOB error ") + theme(legend.position = "bottom")


```
</center>

## <span style="color:#26734d">Simulation 3</span>


* $x_{2,i,g} = \alpha_{g} + \beta x_{1,i} + \epsilon_i$ 
* $\epsilon_i \sim N(0, \sigma^2)$
* $x_{1, i} \sim N(0, 1)$
* $\rho = \frac{\beta}{\sqrt{\beta^2+\sigma^2}}$

The way to control the correlation between $x_1$ and $x_2$ is changing the slope.
The two groups are different because of the intercept.

<!-- \begin{eqnarray*} -->
<!-- \begin{pmatrix}x_{i,1}\\ -->
<!-- x_{i,2} -->
<!-- \end{pmatrix} & \sim & N\left[\left(\begin{array}{c} -->
<!-- 0\\ -->
<!-- 0 -->
<!-- \end{array}\right),\left(\begin{array}{ccc} -->
<!-- 1 & \beta \\ -->
<!-- \beta & \beta^2+\sigma^2 -->
<!-- \end{array}\right)\right]\\ -->
<!-- \rho &=& \frac{\beta}{\sqrt{\beta^2+\sigma^2}}\\ -->
<!-- \beta &=&\sqrt{\frac{\rho^2 \sigma^2}{1-\rho^2}} -->
<!-- \end{eqnarray*} -->


## <span style="color:#26734d">Simulation 3, scenarios</span>

Parameter         | Value                  |
---------         | -------                |
 $\sigma^2$       | $\{1, 10, 20, 30\}$    |
Classes           | $2$                    |
$\beta$ | $\{0,2^1, 2^2,2^3, 2^4, 2^5\}$       |
$\alpha_{g1}$ | 0 |
$\alpha_{g2}$ | $5$ 
Replicates  | $20$                   |
n | 200

    
## <span style="color:#26734d">Simulation 3, data</span>
<center>
```{r, echo = FALSE, fig.height = 6, fig.width = 8, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}

simuFINAL <- function( rep, beta1, beta2,alpha1,alpha2, n, classes, sig){

set.seed(123)
eps <- rnorm(n, sd = sqrt(sig))
x1 <- rnorm(n)
alp <- c( rep(alpha1, each = n/classes), rep(alpha2, each = n/classes))
bet <- c( rep(beta1, each = n/classes), rep(beta2, each = n/classes))
x2 <- alp + bet*x1 + eps

y <- factor(rep(c(1:classes), each=n/classes))

dat1 <- data.frame( y , x1, x2, x3 = rnorm(200), x4 =rnorm(200))

  return(dat1)

}



 opi <- expand.grid( beta1 = c( 0, 2^(1:5) ), beta2 =c( 0, 2^(1:5) ), alpha1= 0, alpha2 = c(5, 10, 20, 30), sig =c(1,10, 20, 30),classes= 2,n = 200,  rep = 1:20)
opisub <- opi %>% filter(beta1 ==  beta2)
 
simu.sceFINAL <- mlply(opisub, simuFINAL)


aux <- ldply(simu.sceFINAL, function(x) 
  x%>%select(x1,x2,y)) %>% filter(rep == 1, alpha2 == 5)


aux  %>%ggplot(  aes( y =x2, x = x1, color = y) ) + geom_point(alpha=.4, size=I(2))  + facet_grid(beta1~sig , labeller = label_both, scale="free_y") + theme(legend.position = "bottom", aspect.ratio = 1)+
  scale_colour_brewer(name="Class",type="qual",palette="Dark2") 
```
</center>

## <span style="color:#26734d">Simulation 3, results </span>
```{r, echo = FALSE, fig.height = 6, fig.width = 6, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}
load("simallFINALbig.Rdata")


mean_all1 <- ddply( all4,.( beta1, beta2, alpha1, alpha2, classes, sig, n,  Model), function(x) mean(x$V1) )
sd_all1 <- ddply( all4,.(beta1, beta2, alpha1, alpha2, classes,sig,  n,  Model), function(x) sd(x$V1) )
pl_all1 <- merge( mean_all1, sd_all1, by = c("beta1",   "beta2" ,  "alpha1" , "alpha2" , "classes" ,"sig",
"n",       "Model") )

pl_all1 %>% filter(alpha2==5)%>%
ggplot(  aes( y = V1.x, x =beta1, color = Model ) ) + geom_line()  + facet_grid(sig~alpha2 , scales = "free_y", labeller = label_both)+geom_errorbar(aes(ymin = V1.x - V1.y, ymax = V1.x + V1.y), width = .1) + labs(x = "Slope", y = "Mean OOB error ") + theme(legend.position = "bottom")
```
</center>

## <span style="color:#26734d">Simulation 3, results </span>
```{r, echo = FALSE, fig.height = 6, fig.width = 6, fig.align = "center", cache = TRUE, warning=FALSE, message=FALSE, echo=FALSE}
pl_all1 %>%filter(alpha2==5)%>% mutate(rho = round(beta1/sqrt(beta1^2+sig), 3)) %>%
ggplot(  aes( y = V1.x, x =rho, color = Model ) ) + geom_line()  + facet_grid(sig~classes , scales = "free_y", labeller = label_both)+geom_errorbar(aes(ymin = V1.x - V1.y, ymax = V1.x + V1.y), width = .1) + labs(x = "Correlation between x1 and x2", y = "Mean OOB error ") + theme(legend.position = "bottom")

```

## <span style="color:#26734d">Info </span>


1.  PPforest package:https://github.com/natydasilva/PPforest
2.  Slides: https://github.com/natydasilva/SlidesPPforestsimulations
3.  Viz : https://natydasilva.shinyapps.io/shinyppforest


## Bibliography

